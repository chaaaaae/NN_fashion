import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, datasets
import random

# 시드 고정 (무작위성 제어)
seed = 42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# Fashion MNIST 데이터 로드 및 전처리
(train_img, train_label), (test_img, test_label) = datasets.fashion_mnist.load_data()
train_img, test_img = train_img / 255.0, test_img / 255.0  # 정규화

# 층별 모델 생성 함수
def create_model(num_layers):
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28)))  # 입력층 (Flatten으로 784 차원으로 변환)
    for _ in range(num_layers):
        model.add(layers.Dense(64, activation='relu'))  # 은닉층
    model.add(layers.Dense(10, activation='softmax'))  # 출력층
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# 층별 오차 저장 리스트
losses = []

# 1층부터 6층까지 신경망 학습 및 오차 저장
for num_layers in range(1, 7):
    model = create_model(num_layers)
    # shuffle=True 옵션 추가
    history = model.fit(train_img, train_label, epochs=5, batch_size=128, shuffle=True, verbose=0, validation_data=(test_img, test_label))
    final_loss = history.history['val_loss'][-1]  # 마지막 에포크의 테스트 손실값
    losses.append(final_loss)
    print(f"{num_layers}층 신경망의 최종 오차: {final_loss}")

# 오차 시각화
plt.figure(figsize=(10, 6))
plt.plot(range(1, 7), losses, marker='o')
plt.title("층별 신경망의 최종 오차 비교")
plt.xlabel("층 수")
plt.ylabel("오차 (Validation Loss)")
plt.xticks(range(1, 7))
plt.grid(True)
plt.show()



""" 미니배치 교차 엔트로피 오차: 0.9018318745193863
교차 엔트로피 오차란?
교차 엔트로피 오차는 모델의 예측이 실제 정답과 얼마나 다른지를 숫자로 나타낸 것입니다.
이 숫자가 클수록 예측이 실제 정답과 많이 다르다는 뜻이고, 작을수록 더 정확하게 예측했다는 뜻이에요.

0.9는 어떤 의미일까?
오차가 0.9라는 것은 예측이 정답에 어느 정도 가깝긴 하지만, 완벽하지는 않다는 것을 뜻해요.
예를 들어, 오차가 0.1 이하로 낮다면 모델의 예측이 아주 정확하다고 볼 수 있어요.
하지만 오차가 0.9 정도라면, 아직 예측이 조금 불안정하고 더 개선될 여지가 있다는 의미입니다.

그러면 0.9는 좋은 걸까, 나쁜 걸까?
0.9는 초반 학습 단계에서는 괜찮은 값입니다.
모델이 처음부터 완벽하게 예측하는 경우는 거의 없고, 학습이 진행될수록 오차가 더 낮아져야 해요.
이 오차가 학습이 진행되면서 점점 줄어든다면, 모델이 점점 더 정확해지고 있다는 긍정적인 신호라고 할 수 있어요.

요약하자면, 0.9라는 오차는 학습 초기에는 괜찮은 값이고, 모델이 점점 학습을 통해 더 낮은 오차를 만들어가는 것이 중요합니다.
"""

"""이것도 미니배치야?
네, 이 코드에서도 미니배치 학습을 사용하고 있습니다. model.fit() 함수에서 batch_size=128로 설정되어 있기 때문에, 매번 전체 데이터를 128개의 샘플로 나누어 학습합니다.

미니배치 학습에서는 각 에포크(epoch)마다 데이터를 여러 개의 미니배치로 나눠서 학습하기 때문에, 데이터가 랜덤하게 섞여서 모델에 입력됩니다. 미니배치의 크기를 지정하지 않으면 **전체 데이터를 한 번에 학습하는 "배치 학습"**이 되는데, 이는 메모리 사용량이 많고 속도가 느려져서 대부분의 딥러닝 모델에서는 미니배치 학습이 사용됩니다.

시드를 고정해도 결과가 달라지는 이유
비록 시드를 고정하여 무작위성을 줄이더라도, 미니배치 학습 특성상 학습 과정에서 미니배치별로 작은 변동이 있을 수 있습니다. 따라서 완벽하게 동일한 결과를 얻기 어렵지만, 시드 고정 덕분에 결과의 변동이 크게 줄어들게 됩니다.

정리하자면: 코드에서 시드를 고정했지만 batch_size=128로 미니배치 학습을 사용하고 있기 때문에, 각 층의 신경망에서 일부 결과가 조금씩 달라질 수 있습니다.
"""

"""랜덤이 아닌데도 미니배치야?
맞습니다, 랜덤이 아닌 고정된 순서로 데이터를 나누더라도 미니배치로 학습을 진행할 수 있습니다. 미니배치는 데이터를 여러 묶음(배치)으로 나누어 학습하는 방식 자체를 의미하기 때문에, 데이터를 무작위로 섞지 않고 정해진 순서로 나눈다 해도 미니배치 학습이 됩니다.

미니배치와 랜덤성의 관계
미니배치: 데이터셋을 작은 묶음으로 나누어 각 묶음에 대해 학습하는 방식입니다. 이때, 데이터가 항상 같은 순서로 들어간다고 하더라도 미니배치 학습은 가능합니다.
랜덤성: 보통은 각 에포크마다 데이터를 섞어서 다양한 배치 구성을 만들어, 모델이 특정 순서에 의존하지 않고 모든 데이터를 고르게 학습하게 합니다. 하지만 데이터를 섞지 않고도 미니배치를 만들 수 있습니다.
즉, 데이터의 순서와 상관없이, 미니배치로 나누어서 학습하는 과정 그 자체가 미니배치 학습입니다.
"""

"""shuffle=True를 포함해서 다시 코드를 만들어줘
네, shuffle=True 옵션을 추가하면 에포크마다 데이터를 무작위로 섞기 때문에 최종 오차 값이 매번 다소 달라질 수 있습니다.

하지만 이전 코드에서 시드를 고정(random.seed, np.random.seed, tf.random.set_seed)해 두었기 때문에, 결과가 크게 변동되지는 않고, 일정 범위 내에서만 약간의 차이가 나는 정도가 될 것입니다. 시드 고정이 없으면 더 큰 변동이 발생할 수 있습니다.

왜 변동이 생기는가?
매 에포크마다 데이터가 섞이면서 미니배치에 포함되는 데이터가 달라져 모델이 학습하는 방향이 조금씩 바뀌기 때문입니다. 그러나 일반적으로 시드를 고정하면 결과가 크게 달라지지는 않습니다.

정리하자면: 시드를 고정했기 때문에 그래프가 약간의 차이만 보이며 안정적으로 나타날 가능성이 높습니다.
"""

"""shuffle=True가 있고 없고의 차이를 중학생에게 설명하듯이 알려줘
shuffle=True가 있는 경우와 없는 경우의 차이를 중학생에게 설명하자면 이렇게 생각하면 쉬울 것 같아요:

shuffle=False: 순서대로 차례차례 학습
shuffle=False라면 모델이 데이터를 항상 같은 순서로 학습해요. 예를 들어, 만약 데이터가 1번부터 100번까지 있다면 첫 번째 학습에는 1번부터 128번까지, 그다음엔 129번부터 256번까지 데이터를 읽으며 학습을 진행해요. 이렇게 되면 모델이 데이터의 특정 순서에 맞춰 학습할 수 있어요.

예를 들어:

만약 처음 데이터들이 모두 티셔츠이고, 그다음 데이터가 신발이라면, 모델은 티셔츠를 먼저 많이 학습하고 나서 신발을 학습해요.
이럴 경우, 모델이 특정 순서에 따라 학습할 가능성이 있어요.
shuffle=True: 매번 섞어서 무작위로 학습
shuffle=True라면 모델이 데이터를 학습할 때 매번 무작위로 섞어서 순서를 다르게 해서 학습해요. 예를 들어, 한 번은 5번, 20번, 42번, 88번 데이터를 먼저 학습하고, 다음엔 7번, 1번, 56번, 33번 데이터를 학습하는 식으로 데이터가 섞여서 들어갑니다.

이렇게 하면 모델이 데이터의 순서에 의존하지 않고, 더 다양한 데이터를 고르게 학습할 수 있어요.

결과 차이
shuffle=False: 데이터 순서에 따라 학습이 진행되어, 학습하는 데이터의 순서에 따라 결과가 조금 달라질 수 있어요.
shuffle=True: 무작위로 섞기 때문에 모델이 데이터 순서에 영향을 받지 않고, 더 균형 잡힌 학습을 할 가능성이 커요.
한마디로 요약: shuffle=True는 모델이 매번 데이터를 무작위로 섞어서 다양한 순서로 학습하게 하여 더 고르게 학습하게 도와줍니다.
"""

"""random.seed, np.random.seed, tf.random.set_seed가 있고 없고의 차이를 중학생에게 설명하듯이 알려줘
random.seed, np.random.seed, tf.random.set_seed가 있는 경우와 없는 경우의 차이를 중학생에게 설명해 볼게요.

컴퓨터에서의 "랜덤"이란?
컴퓨터가 무작위(랜덤)로 숫자를 고르는 것 같지만 사실 완전한 랜덤은 아니에요. 컴퓨터는 규칙에 따라 숫자를 고르기 때문에, 처음 시작 숫자를 정해주면 항상 같은 "랜덤" 결과가 나오도록 할 수 있어요. 이걸 **"시드를 고정한다"**라고 해요.

시드를 고정하지 않으면?
만약 시드를 고정하지 않는다면, 컴퓨터가 매번 다른 무작위 숫자를 선택하게 돼요. 예를 들어, 학습 데이터를 무작위로 섞거나 모델의 가중치를 무작위로 초기화할 때마다 결과가 조금씩 달라질 수 있어요.

예시: 매번 학습할 때마다 결과가 조금씩 달라져요. 모델이 학습할 때 무작위로 데이터가 섞이거나 가중치가 달라지기 때문에, 오차나 정확도가 매번 조금씩 바뀔 수 있어요.
시드를 고정하면?
시드를 고정하면 매번 같은 숫자를 선택하는 규칙을 따르게 되어서, 항상 같은 순서로 무작위 숫자를 선택하게 돼요. 즉, 같은 데이터를 무작위로 섞더라도 결과가 항상 같게 나오고, 모델의 학습 과정도 일정하게 유지돼요.

예시: 시드를 고정하면 학습할 때마다 결과가 일정해져요. 오차나 정확도가 같은 값을 유지하게 되고, 실험을 반복할 때도 매번 같은 결과를 얻을 수 있어요.
세 가지 시드 고정 함수
random.seed: 파이썬에서 일반적인 무작위 숫자를 고정하는 함수예요.
np.random.seed: numpy에서 배열을 무작위로 섞거나 숫자를 고정할 때 사용해요.
tf.random.set_seed: TensorFlow에서 신경망을 학습할 때 무작위로 초기값을 정할 때 사용해요.
이 세 가지를 모두 고정하면, 파이썬의 무작위, numpy의 무작위, TensorFlow의 무작위가 모두 일정하게 고정돼서, 매번 같은 학습 결과를 얻을 수 있어요.

요약
시드를 고정하지 않으면: 무작위 과정이 매번 달라져서 결과가 조금씩 달라져요.
시드를 고정하면: 무작위 과정이 매번 같아져서 결과가 일정해요.
"""

"""뭘로 고정되어 있는거야?
시드를 고정하는 것은 특정 숫자를 사용하여 컴퓨터가 무작위 수를 뽑는 출발점을 고정하는 것이라고 생각하면 됩니다.

예를 들어, random.seed(42)라고 하면, "42"라는 숫자로 고정되는 것입니다. 이 숫자 "42"는 시드의 역할을 하며, 컴퓨터는 이 숫자를 바탕으로 무작위 숫자를 생성하는 규칙을 시작합니다. 그래서 같은 시드 숫자를 사용하면, 매번 같은 "무작위" 결과를 얻게 됩니다.

어떻게 고정되는지 예시
random.seed(42), np.random.seed(42), tf.random.set_seed(42)로 고정해 두면, "42"라는 시드값을 이용해 무작위 수를 뽑기 시작하고, 그 결과는 항상 같게 됩니다.
시드 숫자가 달라지면 (seed(24) 등), 그에 따라 다른 규칙이 시작되므로 다른 무작위 결과가 나옵니다.
한마디로: "42"라는 시드 값을 고정해두면, 컴퓨터는 항상 이 숫자를 기준으로 무작위 값을 계산하여 같은 결과를 얻습니다.
"""

"""어떤 규칙으로 무작위 값을 계산한다는거야?
컴퓨터가 "무작위" 값을 생성할 때 사용하는 규칙은 사실 수학적인 알고리즘을 이용해서 특정한 계산을 반복하는 것입니다. 이 방법을 의사 난수 생성 방식이라고 불러요. 여기서 **의사(pseudo)**는 진짜 무작위는 아니지만 무작위처럼 보인다는 뜻이에요.

의사 난수 생성 방법
컴퓨터는 무작위 값을 계산하기 위해 다음과 같은 규칙을 사용합니다:

시드(seed) 값 설정: 시드는 출발점이 되는 숫자입니다. 예를 들어, "42"라는 시드 값을 사용하면, 이 숫자가 첫 번째 값의 출발점이 됩니다.

수학적 알고리즘 적용: 시드 값에 어떤 특정한 수학 계산을 반복하여, 마치 무작위처럼 보이는 새로운 숫자를 만들어냅니다. 이 알고리즘은 주로 곱셈, 덧셈, 나머지 연산을 조합해서 무작위처럼 보이는 숫자를 생성합니다.

반복해서 새로운 값 생성: 첫 번째로 생성된 값이 다시 다음 값을 계산하는 데 사용됩니다. 이렇게 계산이 계속 반복되면서, 새로운 숫자들이 만들어지고, 이를 통해 무작위처럼 보이는 숫자들이 이어지게 됩니다.

예시
예를 들어, 간단한 규칙으로 이런 방식이 있을 수 있어요:

시드 값이 42라면, 첫 번째 숫자는 (42 * 3 + 7) % 100 = 33이라는 계산을 통해 33이 됩니다.
이 33을 다음 계산에 넣어서 (33 * 3 + 7) % 100 = 6이라는 두 번째 숫자가 나옵니다.
이런 식으로 계속 반복해서 새로운 숫자를 계산합니다.
이 방법을 사용하면, 시드 값이 같다면 항상 같은 숫자들이 반복해서 나오게 됩니다. 하지만 계산된 값들은 마치 무작위로 생성된 것처럼 보입니다.

중요한 점
이 방식 덕분에 같은 시드를 사용하면 항상 같은 무작위 값들이 나오고, 시드를 바꾸면 전혀 다른 숫자들이 생성됩니다. 이렇게 해서 **시드를 통해 무작위 결과를 "고정"**할 수 있게 되는 것입니다.
"""